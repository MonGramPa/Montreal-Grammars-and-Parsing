---
layout: section
title: Noisy channel model
index: 3
use_math: true
bibliography:
- 'variational.bib'
---

Adding in the noisy channel
===========================

Changes to the Generative Model
-------------------------------
Recall the original generative model was:

<center>
$$\begin{equation}
\begin{split}
P(X\mid Z, \Phi)P(Z\mid \Phi) = P(X\mid z', z_a, \Theta, \nu, a, b, \alpha)P(z', z_a', \Theta, \nu \mid G, a,b,\alpha)= \\
P(X\mid z', z_A, G, \nu, \Theta, a, b, \alpha) \\
\times P(z' \mid  z_a, G, \nu, \Theta, a, b, \alpha) \\
\times P(z_a \mid  G, \nu, \Theta, a,b,\alpha) \\
\times P(\nu \mid  a,b)\\
\times P(\Theta\mid \alpha) \end{split}
\end{equation}$$
</center>

Since the actual ``data'' in the joint model are bottom level PLUs, let's change the notation a bit to accommodate an intermediate step (top-level PLUs):
Let:
- $$TOPS$$ be the **top-level PLUs** produced by the adaptor grammar model (written as $$X$$ above)
- $$BOTTOMS$$ be the new data being generated by the model, the **bottom-level PLUs**. 

Because most of the time, we would like identity rewrites -- not a split, deletion, or substitution, we need to limit the number of those operations. We do this by drawing a random variable $$\epsilon$$ from a Beta distribution parametrized by hyperparemeters $$\rho_1, \rho_2$$ with a high bias towards being very small (for example, let $$\rho_1 = 1$$ and $$\rho_2 = 10$$, and choose a non-identity rewrite with $$P= 1-\epsilon$$)

For brevity, let $$\mathcal{T}$$ be the set of nonterminals which expand to bottom-layer PLUs.
We need to re-specify our latent variables and hyperparameters as well.
-   Let $$Z$$ be our set of hidden variable collections: 
    - $$\epsilon$$ = the Beta random variable determining the frequency of rewrites
    - $$\Theta_{top} = \{\Theta_A \mid  A \in \mathcal{N}\setminus \mathcal{T}\}$$: PCFG Multinomials for non-terminal expansions (top layer PLUs)
    - $$\Theta_{bot} = \{\Theta_A \mid A \in \mathcal{T}\}$$: PCFG multinomials for terminal expansions (bottom-layer PLUs).
    - $$\nu = \{\nu_A \mid  A \in \mathcal{M}\}$$: stick length proportions
    - $$z_a = \{z_{A,i} \mid  A \in M, i \in \{1,...\} \}$$: adapted nonterminal's subtrees 
    - $$z' = \{z_A \mid  A \in \mathcal{M}\}$$: the full tree derivations
    - for concise notation, let $$TREES = \{z_a, z'\}$$
- For hyperparameters:
    - add parameters $$\rho_1, \rho_2$$ which parametrize the Beta distribution generating $$\epsilon$$
    - add a parameter $$\alpha_{bot}$$ which parameterizes the Dirichlet process generating $$\Theta_{bot}$$. 
    - add a parameter $$G_{bot}$$ which is the noisy channel grammar, including rewrite, substitution, and deletion rules. 
    - similarly, let $$\alpha$$ in the original model and $$G$$ become $$\alpha_{top}$$ and $$G_{top}$$ respectively.
    - for concise notation, let $$\mathbf{G} = \{G_{top}, G_{bot}\}$$, let $$\mathbf{\alpha} = \{\alpha_{top}, \alpha_{bot}\}$$, and let $$\rho = \{\rho_1, \rho_2\}

First, let's rewrite our original generative model in terms of these new variables:

<center>
$$\begin{equation}
\begin{split}
P(TOPS, Z, \Phi) = P(TOPS \mid Z, \Phi) P(Z \mid \Phi) = \\
P(TOPS \mid TREES, \Theta_{bot}, \Theta_{top}, \nu, a, b, \mathbf{G}, \mathbf{\alpha}) \\
\times P(TREES \mid \Theta_{bot}, \Theta_{top}, \nu, a, b, \mathbf{G}, \mathbf{\alpha}) \\
\times P(\Theta_{bot} \mid \alpha_{bot}) \\
\times P(\Theta_{top} \mid \alpha_{top}) \\
\times P(\nu \mid  a,b)
\end{split}
\end{equation}$$
</center>



Now in addition, we are adding another layer to the top, namely $$P(BOTTOMS \mid TOPS, Z, \Phi)$$. If we think about what $$BOTTOMS$$ depends on, we can expand this to get 
<center>
$$\begin{equation}
\begin{split}
P(BOTTOMS| \epsilon, TOPS, TREES,\Theta_{top},\Theta_{bot}, \nu, a, b, \mathbf{G}, \mathbf{\alpha}, \rho) \\
\times  P(\Theta_{top}|\alpha_{top})\\
\times P(TOPS \mid TREES, \Theta_{top},\Theta_{bot}, \nu, a, b,  \mathbf{G}, \mathbf{\alpha})\\
\times P(TREES, \Theta_{top}, \Theta_{bot}, \nu | a, b, \mathbf{G}, \mathbf{\alpha})
\times P(\epsilon | \rho_1, \rho_2)
\end{split}
\end{equation}
$$
</center>

Thankfully, the last part is just our original generative model, so the whole model rewrites to:

<center>
$$\begin{equation}
\begin{split}
P(BOTTOMS, TOPS, Z | \Phi)= P(BOTTOMS\mid TOPS, Z, \Phi)P(TOPS \mid Z, \Phi)P(Z\mid \Phi) =\\
 P(BOTTOMS \mid TOPS, TREES, \Theta_{bot}, \Theta_{top}, \nu, \Phi)P(TOPS \mid TREES, \Theta_{bot}, \Theta_{tops}, \nu, \Phi) = \\
 P(BOTTOMS \mid TOPS, TREES, \Theta_{bot}, \Theta_{top}, \nu, a, b, \mathbf{G}, \mathbf{\alpha}) \\
\times P(TOPS \mid TREES, \Theta_{bot}, \Theta_{top}, \nu, a, b, \mathbf{G}, \mathbf{\alpha}) \\
\times P(TREES \mid \Theta_{bot}, \Theta_{top}, \nu, a, b, \mathbf{G}, \mathbf{\alpha}) \\
\times P(\Theta_{bot} \mid \alpha_{bot}) \\
\times P(\Theta_{top} \mid \alpha_{top}) \\
\times P(\nu \mid  a,b)
\end{split}
\end{equation}$$
</center>


ELBO
----
Recall that our ELBO before was:


<center>
$$\begin{equation}\begin{split}
= \mathcal{H}(q) + \sum\limits_{A\in\mathcal{M}} \mathbb{E}_q[\log\ p(\Theta \mid  \alpha)] \\
+ \sum\limits_{A\in\mathcal{M}} \mathbb{E}_q[\log\ p(\nu \mid  a) ]\\
+ \sum\limits_{A\in\mathcal{M}} \mathbb{E}_q[\log\ p(z_A \mid  \nu, \Theta)] \\
+ \sum\limits_{A\in\mathcal{M}} \mathbb{E}_q[\log\ p(x, z' \mid  \nu_A)] \\
\end{split}\end{equation}$$
</center>

We need to factor the noisy channel model into this. 


<center>
$$\begin{equation}\begin{split}
= \mathcal{H}(q) + \\
+ \sum\limits_{A \in \mathcal{T}} \mathbb{E}_q[\log\ p(\epsilon \mid \rho_1, \rho_2)]
+ \sum\limits_{A \in \mathcal{T}} \mathbb{E}_q[\log\ p(\Theta_{bot} \mid  \alpha_{bot}] \\
+ \sum\limits_{A\in\mathcal{M} \setminus \mathcal{T}}  \mathbb{E}_q[\log\ p(\Theta_{top} \mid  \alpha_{top}] \\
+ \sum\limits_{A\in\mathcal{M} \setminus \mathcal{T}} \mathbb{E}_q[\log\ p(\nu \mid  a, b) ]\\
+ \sum\limits_{A\in\mathcal{M}} \mathbb{E}_q[\log\ p(z_A \mid  \nu, \Theta_{bot}, \Theta_{top}, \epsilon)] \\
+ \sum\limits_{A\in\mathcal{M} \setminus \mathcal{T}} \mathbb{E}_q[\log\ p(x, z' \mid  \nu_A)] \\
\end{split}\end{equation}$$
</center>


Updates
-------
Applying the variational methodology to this model, since we only have one added latent variable ($$\Theta_{bot}$$) we need only to find one new variational distribution. Call this distribution $$q_{\tau_n}(\Theta_{bot})$$. The update for this parameter $$\tau_n$$ will be very similar to the update of the variational parameter indexed by $$\Theta_{top}$$ in the original model:

<center>
$$\begin{equation}
\begin{split}
\tau_{n, A\rightarrow \beta} = \sum\limits_{B\in \mathcal{T}} \sum\limits_{k=1}^{N_B} \tilde{f}(A\rightarrow \beta, S_{B,k})
\end{split}
\end{equation}$$
</center>

where $$\tilde{f}(A\rightarrow \beta, S_{B,k})$$ is the expected number of times the rule $$A\rightarrow \beta$$ appears in the derivation $$S_{B,k}$$



