---
layout: section
title: Noisy channel model
index: 3
use_math: true
bibliography:
- 'variational.bib'
---

Adding in the noisy channel
===========================

Changes to the Generative Model
-------------------------------
Recall the original generative model was:

<center>
$$\begin{aligned}
P(X\mid Z, \Phi)P(Z\mid \Phi) = P(X\mid z', z_a, \Theta, \nu, a, b, \alpha)P(Z\mid G, a,b,\alpha)= \\
P(X\mid z', z_A, G, \nu, \Theta, a, b, \alpha) \\
\times P(z' \mid  z_a, G, \nu, \Theta, a, b, \alpha) \\
\times P(z_a \mid  G, \nu, \Theta, a,b,\alpha) \\
\times P(\nu \mid  a,b)\\
\times P(\Theta\mid \alpha) \end{aligned}$$
</center>

Since the actual ``data'' in the joint model are top level PLUs, let's change the notation a bit for the sake of consistency:
Let:
    - $$Y$$ be the **bottom-level PLUs** produced by the adaptor grammar model (written as $$X$$ above)
    - $$X$$ be the new data being generated by the model, the **top-level PLUs**. 

For brevity, let $$\mathcal{T}$$ be the set of nonterminals which expand to top-layer PLUs.
We need to re-specify our latent variables and hyperparameters as well.
-   Let $$Z$$ be our set of hidden variable collections: 
    - $$\Theta_{bot} = \{\Theta_A \mid  A \in \mathcal{N}-\mathcal{T}\}$$: PCFG Multinomials for non-terminal expansions (bottom layer PLUs)
    - $$\Theta_{top} = \{\Theta_A \mid A \in \mathcal{T}\}$$: PCFG multinomials for terminal expansions (top-layer PLUs).
    - $$\nu = \{\nu_A \mid  A \in \mathcal{M}\}$$: stick length proportions
    - $$z_a = \{z_{A,i} \mid  A \in M, i \in \{1,...\} \}$$: adapted nonterminal's subtrees 
    - $$z' = \{z_A \mid  A \in \mathcal{M}\}$$: the full tree derivations
- For hyperparameters:
    - add a parameter $$\alpha_{top}$$ which parameterizes the Dirichlet process generating $$\Theta_{top}$$. 
    - add a parameter $$G_{top}$$ which is the noisy channel grammar, including rewrite, substitution, and deletion rules. 
    - similarly, let $$\alpha$$ in the original model and $$G$$ become $$\alpha_{bot}$$ and $$G_{bot}$$ respectively.


First, let's rewrite our original generative model in terms of these new variables:

<center>
$$\begin{aligned}
P(Y\mid Z, \Phi)P(Z\mid \Phi) = P(Y\mid z', z_a, \Theta_{bot}, \nu, a, b, \alpha_{bot})P(Z\mid G_{bot}, a,b,\alpha_{bot})= \\
P(X\mid z', z_A, G_{bot}, \nu, \Theta_{bot}, a, b, \alpha_{bot}) \\
\times P(z' \mid  z_a, G_{bot}, \nu, \Theta_{bot}, a, b, \alpha_{bot}) \\
\times P(z_a \mid  G_{bot}, \nu, \Theta_{bot}, a,b,\alpha_{bot}) \\
\times P(\nu \mid  a,b)\\
\times P(\Theta_{bot}\mid \alpha_{bot}) \end{aligned}$$
</center>

Now in addition, we are adding another layer to the top, namely $$P(X|Y, Z, \Phi)$$. If we think about what $$X$$ depends on, we can expand this to get 
<center>
$$\begin{aligned}
P(X | Y, G_{top}, \Theta_{top}, \alpha_{top}) \\
\times  P(\Theta_{top}|\alpha_{top})\\
\times P(Y|Z, \Phi)P(Z | \Phi)
\end{aligned}
$$
</center>

Thankfully, the last part is just our original generative model, so the whole model rewrites to:

<center>
$$\begin{aligned}
P(X | Y, G_{top}, \Theta_{top}, \alpha_{top}) \\
\times  P(\Theta_{top}|\alpha_{top})\\
P(Y\mid z', z_a, \Theta_{bot}, \nu, a, b, \alpha_{bot})P(Z\mid G_{bot}, a,b,\alpha_{bot})= \\
P(X\mid z', z_A, G_{bot}, \nu, \Theta_{bot}, a, b, \alpha_{bot}) \\
\times P(z' \mid  z_a, G_{bot}, \nu, \Theta_{bot}, a, b, \alpha_{bot}) \\
\times P(z_a \mid  G_{bot}, \nu, \Theta_{bot}, a,b,\alpha_{bot}) \\
\times P(\nu \mid  a,b)\\
\times P(\Theta_{bot}\mid \alpha_{bot})

\end{aligned}
$$
</center>


Updates
-------
Applying the variational methodology to this model, since we only have one added latent variable ($$\Theta_{top}$$) we need only to find one new variational distribution. Call this distribution $$q_{\tau_n}(\Theta_{top})$$. The update for this parameter $$\tau_n$$ will be very similar to the update of the variational parameter indexed by $$\Theta_{bot}$$ in the original model:

<center>
$$\begin{aligned}
\tau_{n, A\rightarrow \beta} = \sum\limits_{B\in \mathcal{T}} \sum\limits_{k=1}^{N_B} \tilde{f}(A\rightarrow \beta, S_{B,k})
\end{aligned}$$
</center>

where $$\tilde{f}(A\rightarrow \beta, S_{B,k})$$ is the expected number of times the rule $$A\rightarrow \beta$$ appears in the derivation $$S_{B,k}$$