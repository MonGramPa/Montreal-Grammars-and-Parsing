<h1 id="adding-in-the-noisy-channel">Adding in the noisy channel</h1>

<h2 id="changes-to-the-generative-model">Changes to the Generative Model</h2>
<p>Recall the original generative model was:</p>

<center>
$$\begin{equation}
\begin{split}
P(X\mid Z, \Phi)P(Z\mid \Phi) = P(X\mid z', z_a, \Theta, \nu, a, b, \alpha)P(z', z_a', \Theta, \nu \mid G, a,b,\alpha)= \\
P(X\mid z', z_A, G, \nu, \Theta, a, b, \alpha) \\
\times P(z' \mid  z_a, G, \nu, \Theta, a, b, \alpha) \\
\times P(z_a \mid  G, \nu, \Theta, a,b,\alpha) \\
\times P(\nu \mid  a,b)\\
\times P(\Theta\mid \alpha) \end{split}
\end{equation}$$
</center>

<p>Since the actual ``data’’ in the joint model are bottom level PLUs, let’s change the notation a bit to accommodate an intermediate step (top-level PLUs):
Let:</p>
<ul>
  <li><script type="math/tex">TOPS</script> be the <strong>top-level PLUs</strong> produced by the adaptor grammar model (written as <script type="math/tex">X</script> above)</li>
  <li><script type="math/tex">BOTTOMS</script> be the new data being generated by the model, the <strong>bottom-level PLUs</strong>.</li>
</ul>

<p>For brevity, let <script type="math/tex">\mathcal{T}</script> be the set of nonterminals which expand to top-layer PLUs.
We need to re-specify our latent variables and hyperparameters as well.</p>
<ul>
  <li>Let <script type="math/tex">Z</script> be our set of hidden variable collections:
    <ul>
      <li><script type="math/tex">\Theta_{bot} = \{\Theta_A \mid  A \in \mathcal{N}\setminus \mathcal{T}\}</script>: PCFG Multinomials for non-terminal expansions (bottom layer PLUs)</li>
      <li><script type="math/tex">\Theta_{top} = \{\Theta_A \mid A \in \mathcal{T}\}</script>: PCFG multinomials for terminal expansions (top-layer PLUs).</li>
      <li><script type="math/tex">\nu = \{\nu_A \mid  A \in \mathcal{M}\}</script>: stick length proportions</li>
      <li><script type="math/tex">z_a = \{z_{A,i} \mid  A \in M, i \in \{1,...\} \}</script>: adapted nonterminal’s subtrees</li>
      <li><script type="math/tex">z' = \{z_A \mid  A \in \mathcal{M}\}</script>: the full tree derivations</li>
    </ul>
  </li>
  <li>For hyperparameters:
    <ul>
      <li>add a parameter <script type="math/tex">\alpha_{top}</script> which parameterizes the Dirichlet process generating <script type="math/tex">\Theta_{top}</script>.</li>
      <li>add a parameter <script type="math/tex">G_{top}</script> which is the noisy channel grammar, including rewrite, substitution, and deletion rules.</li>
      <li>similarly, let <script type="math/tex">\alpha</script> in the original model and <script type="math/tex">G</script> become <script type="math/tex">\alpha_{bot}</script> and <script type="math/tex">G_{bot}</script> respectively.</li>
    </ul>
  </li>
</ul>

<p>First, let’s rewrite our original generative model in terms of these new variables:</p>

<center>
$$\begin{equation}
\begin{split}
P(BOTTOMS\mid Z, \Phi)P(Z\mid \Phi) = P(BOTTOMS \mid z', z_a, \Theta_{bot}, \nu, a, b, \alpha_{bot})P(Z\mid G_{bot}, a,b,\alpha_{bot})= \\
P(X\mid z', z_A, G_{bot}, \nu, \Theta_{bot}, a, b, \alpha_{bot}) \\
\times P(z' \mid  z_a, G_{bot}, \nu, \Theta_{bot}, a, b, \alpha_{bot}) \\
\times P(z_a \mid  G_{bot}, \nu, \Theta_{bot}, a,b,\alpha_{bot}) \\
\times P(\nu \mid  a,b)\\
\times P(\Theta_{bot}\mid \alpha_{bot}) \end{split}
\end{equation}$$
</center>

<p>Now in addition, we are adding another layer to the top, namely <script type="math/tex">P(X|Y, Z, \Phi)</script>. If we think about what <script type="math/tex">X</script> depends on, we can expand this to get</p>
<center>
$$\begin{equation}
\begin{split}
P(X | Y, G_{top}, \Theta_{top}, \alpha_{top}) \\
\times  P(\Theta_{top}|\alpha_{top})\\
\times P(Y|Z, \Phi)P(Z | \Phi)
\end{split}
\end{equation}
$$
</center>

<p>Thankfully, the last part is just our original generative model, so the whole model rewrites to:</p>

<center>
$$\begin{equation}
\begin{split}
P(X | Y, G_{top}, \Theta_{top}, \alpha_{top}) \\
\times  P(\Theta_{top}|\alpha_{top})\\
P(Y\mid z', z_a, \Theta_{bot}, \nu, a, b, \alpha_{bot})P(Z\mid G_{bot}, a,b,\alpha_{bot})= \\
P(X\mid z', z_A, G_{bot}, \nu, \Theta_{bot}, a, b, \alpha_{bot}) \\
\times P(z' \mid  z_a, G_{bot}, \nu, \Theta_{bot}, a, b, \alpha_{bot}) \\
\times P(z_a \mid  G_{bot}, \nu, \Theta_{bot}, a,b,\alpha_{bot}) \\
\times P(\nu \mid  a,b)\\
\times P(\Theta_{bot}\mid \alpha_{bot})

\end{split}
\end{equation}
$$
</center>

<h2 id="updates">Updates</h2>
<p>Applying the variational methodology to this model, since we only have one added latent variable (<script type="math/tex">\Theta_{top}</script>) we need only to find one new variational distribution. Call this distribution <script type="math/tex">q_{\tau_n}(\Theta_{top})</script>. The update for this parameter <script type="math/tex">\tau_n</script> will be very similar to the update of the variational parameter indexed by <script type="math/tex">\Theta_{bot}</script> in the original model:</p>

<center>
$$\begin{equation}
\begin{split}
\tau_{n, A\rightarrow \beta} = \sum\limits_{B\in \mathcal{T}} \sum\limits_{k=1}^{N_B} \tilde{f}(A\rightarrow \beta, S_{B,k})
\end{split}
\end{equation}$$
</center>

<p>where <script type="math/tex">\tilde{f}(A\rightarrow \beta, S_{B,k})</script> is the expected number of times the rule <script type="math/tex">A\rightarrow \beta</script> appears in the derivation <script type="math/tex">S_{B,k}</script></p>
