---
layout: chapter
title: Inside-Outside Algorithm
description: Manning and Schutze (1999:ch.11)
---

#### Manning, Christopher and Hinrich Sch√ºtze (1999). *Foundations of Statistical Natural Language Processing*. MIT Press.

#### Notes by Chris Bruno, May 28, 2017

## Background

**Definition.** A PCFG is a 5-tuple $$(\mathbf{W}, \mathbf{N}, \mathbf{R}, S, P)$$ where
- $$\mathbf{W} = \{w^1, \ldots, w^k\}$$ is a set of terminals.
- $$\mathbf{N} = \{N^1, \ldots, N^n\}$$ is a set of non-terminals.
- $$\mathbf{R} = \{A \to (\mathbf{W} \cup \mathbf{N})^* : A \in \mathbf{N}\}$$ is a set of rules.
- $$S \in \mathbf{N}$$ is the start symbol.
- $$P$$ is a set of probabilities such that for each $$A \in \mathbf{N}$$, $$\sum_{A \to \zeta \in \mathbf{R}}P(A \to \zeta) = 1$$.

**Notation.**
- Given a string $$W \in \mathbf{W}^*$$ of length $$m$$, let $$w_{pq}$$ be the substring from index $$p$$ to $$q$$ (so that $$w_{1m} = W$$).
- For any $$A \in \mathbf{N}$$, $$A_{pq}$$ expresses that there is a constituent rooted at $$A$$ from  index $$p$$ to $$q$$.
- $$A \Longrightarrow w_{pq}$$ expresses that there is some finite sequence of rules starting with nonterminal $$A$$ that generates the substring $$w_{pq}$$.

A PCFG encodes these three **assumptions**:
1. **Place invariance.** The probability of a subtree does not depend on where in the string the words it dominates are:

    For any $$A \in \mathbf{N}$$ and indices $$k,c: P(A_{k(k+c)})$$ are the same.

2. **Context free.** The probability of a subtree doesn't depend on words not dominated by the subtree:

    For any $$A \in \mathbf{N}$$, indices $$k, l$$ and any substring $$s$$ outside $$w_{kl}: P(A_{kl} \to \zeta \mid s) = P(A_{kl})$$.

3. **Ancestor-free.** The probability of a subtree doesn't depend on any of its ancestors.

    For any $$A \in \mathbf{N}$$ and any ancestor node $$\nu: P(A \to \zeta \mid \nu) = P(A \to \zeta)$$

**Three important questions.**
1. What is the probability of a sentence $$w_{1m}$$ given $$G$$?

    $$P(w_{1m} \mid G)$$
    --- (Inside-algorithm)

2. What is the most likely parse for a sentence?

    $$\arg\max_tP(t\mid w_{1m},G)$$
    --- (modification of the Inside-algorithm)

3. How to choose rule probabilities for a grammar $$G$$ which maximizes the probability of an untagged corpus $$W$$.

    $$\arg\max_GP(W\mid G)$$
    --- (Inside-outside algoritm)


**Chomsky Normal Form.** Assume, for the following algorithms, that our grammars are in CNF: all rules are of one of two forms:
1. $$A \to B C$$ where $$B, C \in \mathbf{N}$$
2. $$A \to w$$ where $$w \in \mathbf{W}$$

If $$G$$ is in CNF then for all $$A \in \mathbf{N}$$:
$$\displaystyle\sum_{B,C \in \mathbf{N}}P(A \to B C) + \sum_{k=1}^V P(A \to w^k) = 1$$.

## Inside and outside probabilities

Given a sentence $$W = w_{1m}$$, define:

**Outside probabilities.** $$\alpha_A(p, q) = P(w_{1(p-1)}, A_{pq}, w_{(q+1)m}\mid G)$$

The probability that substring $$w_{pq}$$ is a constituent with root category $$A$$.

**Inside probabilities.** $$\beta_A(p, q) = P(w_{pq} \mid A_{pq}, G)$$

The probability that subtring $$w_{pq}$$ is generated by some sequence of rules given that its root category is $$A$$.

## Inside algorithm

**Input.** A sentence $$W = w_{1m}$$ and PCFG $$G$$ in CNF.

**Base case.** For all $$k, 1 \leq k \leq m: \beta_A(k, k) = P(A \to w_k \mid G)$$

**Induction.** Let $$p \lt q$$ where $$1 \leq p, q \leq m$$.

Since $$G$$ is in CNF, if $$A$$ spans $$p,q$$ then $$A$$ must rewrite to two nonterminals $$B, C$$. Then,
$$\displaystyle\beta_A(p, q) = \sum_{B,C \in \mathbf{N}}\sum_{d=p}^{q-1}P(A \to B C)\beta_B(p, d)\beta_C(d+1, q)$$


## Outside algorithm

**Input.** A sentence $$W = w_{1m}$$ and PCFG $$G$$ in CNF.

**Base case.**
- Let $$\alpha_A(1, m) = 1$$  
- Let $$\alpha_X(1, m) = 0$$ for all $$X \in \mathbf{N}, X \neq A$$

**Induction.**  Sum together two possibilities:
$$\displaystyle\alpha_A(p, q) =
\sum_{B, C \in \mathbf{N}; B, C \neq A}\sum_{c=q+1}^m \alpha_C(p,e)P(C \to A B)\beta_B(q+1, e) +
\sum_{B, C \in \mathbf{N}}\sum_{c=1}^{p-1}\alpha_C(e, q)P(C \to B A)\beta_B(e, p-1)$$


## Finding the most likely parse for a sentence

Alter the Inside algorithm, replacing sums with max.

**Input.** A sentence $$W = w_{1m}$$ and PCFG $$G$$ in CNF.

**Base case.** For all $$k, 1 \leq k \leq m$$:
$$\displaystyle\delta_A(k, k) = P(A \to w_k \mid G)$$

**Induction.** For all $$A \in \mathbf{N}$$ and for all $$p,q$$ where $$1 \leq p, q \leq m$$ and $$p \lt q$$:
$$
\displaystyle\delta_A(p, q) = \max_{B,C \in \mathbf{N}; p \leq d \lt q}P(A \to B C)\delta_A(p, d)\delta_A(d+1, q)$$
,$$\displaystyle\psi_A(p, q) = \arg\max_{B,C \in \mathbf{N}; p \leq d \lt q}P(A \to B C)\psi_A(p, d)\psi_A(d+1, q)$$

**Backtrack.** The probability of the most likely parse tree is $$\delta_S(1, m)$$. Then expand from top to bottom as follows:
- Let $$\nu$$ be a node spanning indices $$p$$ to $$q$$ whose root category is $$A$$.
- If $$p \lt q$$ then $$\psi_A(p, q) = (B, C, d)$$ and:
    - $$\text{left}(\nu)$$ has category $$B$$ and spans $$(p, d)$$
    - $$\text{right}(\nu)$$ has category $$C$$ and spans $$(d+1, q)$$
    - Recurse on both children.
- If $$p = q$$ then $$A$$ rewrites to $$w_p$$.

## Training a PCFG: the Inside-Outside EM algorithm

### Motivation

We would like to calculate
$$\hat{P}(A \to \zeta) = \frac{c(A \to \zeta)}{\sum_\gamma c(A \to \gamma)}$$
for all rules $$A \to \zeta \in \mathbf{N}$$.

Without a tagged corpus, this cannot be calculated directly.
We show how to calculate this given a single sentence, then extend it to a corpus of sentences. Assume we are given a sentence $$W = w_{1m}$$.

We have $$\displaystyle P(A \Longrightarrow w_{pq} \mid S \Longrightarrow w_{1m}, G) = \frac{\alpha_A(p,q)\beta_A(p,q)}{\pi}$$,
where $$\pi = P(S \Longrightarrow w_{1m})$$.

If we sum over all strings, we get the expected number of times $$A$$  is used in  the derivation of $$w_{1m}$$:
$$\displaystyle E(A \text{ is used in } S \Longrightarrow w_{1m}) =
\sum_{p=1}^m\sum_{q=p}^m \frac{\alpha_A(p,q)\beta_A(p,q)}{\pi}$$

Furthermore, since $$G$$ is in CNF, we can flesh out two cases of $$\displaystyle P(A \Longrightarrow w_{pq} \mid S \Longrightarrow w_{1m}, G)$$.
1. If $$A$$ expands to two nonterminals $$B, C$$ then
$$\displaystyle P(A \to B C \Longrightarrow w_{pq} \mid S \Longrightarrow w_{1m}, G) =
\sum_{d=p}^{q-1} \frac{\alpha_A(p,q)P(A \to B C)\beta_B(p, d)\beta_C(d + 1, q)}{\pi}$$

    Then we can sum over all ranges of words that the node $$A$$ could dominate to get the expected number of times the rule is used

    $$\displaystyle E(A \to B C \text{  is used in } S \Longrightarrow w_{1m}) =
    \sum_{p=1}^{m-1}\sum_{q=p+1}^m\sum_{d=p}^{q-1} \frac{\alpha_A(p,q)P(A \to B C)\beta_B(p, d)\beta_C(d + 1, q)}{\pi}$$

2. If $$A$$ expands to a terminal $$w^k$$ then
$$\displaystyle P(A \to w^k \mid S \Longrightarrow w_{1m}, G) =
\sum_{h=1}^{m} \frac{\alpha_A(h,h)P(A \to w_h, w_h = w^k) }{\pi}=
\sum_{h=1}^{m} \frac{\alpha_A(h,h)P(w_h = w^k)\beta_A(h,h) }{\pi}$$


Then we use these three equations to get estimated probabilities of all the rules. There are two cases:
1. $$\hat{P}(A \to B C) = \frac{E(A \to B C \text{  is used in } S \Longrightarrow w_{1m})}{E(A \text{ is used in } S \Longrightarrow w_{1m})}$$
2. $$\hat{P}(A \to w^k) = \frac{P(A \to w^k \mid S \Longrightarrow w_{1m}, G)}{E(A \text{ is used in } S \Longrightarrow w_{1m})}$$

Finally, we extend this to an entire corpus. Let $$W = \langle W^{(1)}, \ldots, W^{(\omega)}\rangle$$ be a corpus of $$\omega$$ sentences, where each sentence $$W^{(i)} = w^{(i)}_{1m_i} = w^{(i)}_1\cdots w^{(i)}_{m_i}$$. Then we sum over all sentences as such:
1. $$\hat{P}(A \to B C) = \frac{\sum_{i=1}^\omega E(A \to B C \text{  is used in } S \Longrightarrow w_{1m})}{\sum_{i=1}^\omega E(A \text{ is used in } S \Longrightarrow w_{1m})}$$
2. $$\hat{P}(A \to w^k) = \frac{\sum_{i=1}^\omega P(A \to w^k \mid S \Longrightarrow w_{1m}, G)}{\sum_{i=1}^\omega E(A \text{ is used in } S \Longrightarrow w_{1m})}$$

So the final equations are
1. $$\hat{P}(A \to B C) = \frac{\sum_{i=1}^\omega \sum_{p=1}^{m-1}\sum_{q=p+1}^m\sum_{d=p}^{q-1} \alpha_A(p,q)P(A \to B C)\beta_B(p, d)\beta_C(d + 1, q)}{\sum_{i=1}^\omega \sum_{p=1}^m\sum_{q=p}^m \alpha_A(p,q)\beta_A(p,q)}$$
2. $$\hat{P}(A \to w^k) = \frac{\sum_{i=1}^\omega \sum_{h=1}^{m} \alpha_A(h,h)P(w_h = w^k)\beta_A(h,h) }{\sum_{i=1}^\omega \sum_{p=1}^m\sum_{q=p}^m \alpha_A(p,q)\beta_A(p,q)}$$

### Procedure

**Input.** A corpus $$W$$ and a PCFG $$G_0$$ whose probabilities have been pre-initialized to some values.

**Procedure.** Repeat $$i = 1, \ldots$$:
- Initialize $$G_i \leftarrow G_{i-1}$$
- For all rules $$\rho \in \mathbf{R}$$:
    - Compute $$\hat{P}(\rho)$$
    - Update probabilities of $$G_i$$ with $$\hat{P}(\rho)$$
